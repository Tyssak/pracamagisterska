{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":9004236,"sourceType":"datasetVersion","datasetId":5424451}],"dockerImageVersionId":30588,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"### Importing Necessary Libraries","metadata":{}},{"cell_type":"code","source":"import pandas as pd  # For data manipulation and analysis\nimport numpy as np  # For numerical operations and working with arrays\nimport cv2 as cv  # For image processing tasks\nimport tensorflow as tf  # For building and training neural network models\nimport keras\n\nfrom keras.preprocessing.image import ImageDataGenerator  # For real-time data augmentation\nfrom tensorflow.keras.models import load_model  # For loading a saved Keras model\nfrom keras.models import Sequential  # For creating a linear stack of layers in the model\nfrom keras.layers import Conv2D, MaxPooling2D, Dense, Dropout, Flatten  # For building model layers\nfrom keras.optimizers import Adam  # For optimization algorithms\nfrom keras.layers import BatchNormalization  # For applying Batch Normalization in neural network layers\nfrom keras.regularizers import l2  # For applying L2 regularization to prevent overfitting\nfrom keras.callbacks import ReduceLROnPlateau, EarlyStopping, TensorBoard, ModelCheckpoint  # Importing specific callback functions\nfrom sklearn.utils import resample, compute_class_weight\nimport warnings  # For handling warnings\nimport sys  # For interacting with the Python interpreter\nif not sys.warnoptions:\n    warnings.simplefilter(\"ignore\")  # Ignore simple warnings if not already done\nwarnings.filterwarnings(\"ignore\", category=DeprecationWarning)  # Ignore deprecation warnings\n\nprint(\"TensorFlow version:\", tf.__version__)\nprint(\"Keras version:\", keras.__version__)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-08-18T12:02:51.269376Z","iopub.execute_input":"2024-08-18T12:02:51.270265Z","iopub.status.idle":"2024-08-18T12:03:06.487001Z","shell.execute_reply.started":"2024-08-18T12:02:51.270215Z","shell.execute_reply":"2024-08-18T12:03:06.485943Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n","output_type":"stream"},{"name":"stdout","text":"TensorFlow version: 2.13.0\nKeras version: 2.13.1\n","output_type":"stream"}]},{"cell_type":"code","source":"import os # For interacting with the operating system, like file paths\nimport shutil # For coping image files and separating training and testing directories \n\ndef clear_working_directory(directory):\n    for root, dirs, files in os.walk(directory):\n        for file in files:\n            os.remove(os.path.join(root, file))\n        for dir in dirs:\n            shutil.rmtree(os.path.join(root, dir))\n\nclear_working_directory('/kaggle/working')","metadata":{"execution":{"iopub.status.busy":"2024-08-18T12:03:06.489068Z","iopub.execute_input":"2024-08-18T12:03:06.489654Z","iopub.status.idle":"2024-08-18T12:03:06.496634Z","shell.execute_reply.started":"2024-08-18T12:03:06.489626Z","shell.execute_reply":"2024-08-18T12:03:06.495610Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"# Adjust these to your requirements \nnr_classes = 7 # number of classes in dataset\ndataset_size = (227, 227) # (227, 227) for AlexNet, (48,48) for CNN2D\nnr_of_channels = 3 # 3 for RGB datasets (default), 1 for grayscale dataset\nbatch_size = 64 # 64 for most datasets seems optimal. Lower value for some small datasets\ndataset_path = '/kaggle/input/laczone/wszystkie' # path to dataset\nnr_epochs = 30 # number of epochs","metadata":{"execution":{"iopub.status.busy":"2024-08-18T12:03:06.497942Z","iopub.execute_input":"2024-08-18T12:03:06.498276Z","iopub.status.idle":"2024-08-18T12:03:06.544218Z","shell.execute_reply.started":"2024-08-18T12:03:06.498248Z","shell.execute_reply":"2024-08-18T12:03:06.543172Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"### To uncomment if dataset requires spliting to test and train directory","metadata":{}},{"cell_type":"code","source":"# # Define the path to your original dataset and the paths where you want to store your train and test datasets\n# original_dataset_dir = '/kaggle/input/ravdessstrong/strong_filtered'\n# train_dir = 'CK+48_train'\n# test_dir = 'CK+48_test'\n\n# # Create directories for training and testing datasets if they do not exist\n# os.makedirs(train_dir, exist_ok=True)\n# os.makedirs(test_dir, exist_ok=True)\n\n# # Define the split ratio\n# #train_ratio = 0.8\n# test_ratio = 0.2\n\n# # Loop through each emotion category in the original dataset\n# for emotion in os.listdir(original_dataset_dir):\n#     emotion_dir = os.path.join(original_dataset_dir, emotion)\n#     if os.path.isdir(emotion_dir):\n#         # Get a list of all the image filenames in the emotion category\n#         images = [f for f in os.listdir(emotion_dir) if os.path.isfile(os.path.join(emotion_dir, f))]\n#         sorted_images = sorted(images)\n#         #sorted_images = p.random.shuffle(images)\n        \n#         # Split the list of image filenames into training and testing sets\n# #         train_size = int(len(sorted_images) * train_ratio)\n# #         train_images = sorted_images[:train_size]\n# #         test_images = sorted_images[train_size:]\n#         test_size = int(len(sorted_images) * test_ratio)\n#         test_images = sorted_images[:test_size]\n#         train_images = sorted_images[test_size:]\n        \n#         # Create directories for the emotion category in the train and test datasets\n#         train_emotion_dir = os.path.join(train_dir, emotion)\n#         test_emotion_dir = os.path.join(test_dir, emotion)\n#         os.makedirs(train_emotion_dir, exist_ok=True)\n#         os.makedirs(test_emotion_dir, exist_ok=True)\n        \n#         # Copy the images into the corresponding directories\n#         for image in train_images:\n#             shutil.copy(os.path.join(emotion_dir, image), os.path.join(train_emotion_dir, image))\n#         for image in test_images:\n#             shutil.copy(os.path.join(emotion_dir, image), os.path.join(test_emotion_dir, image))\n\n# print(\"Dataset splitting complete\")","metadata":{"execution":{"iopub.status.busy":"2024-08-18T12:03:06.547699Z","iopub.execute_input":"2024-08-18T12:03:06.548123Z","iopub.status.idle":"2024-08-18T12:03:06.560776Z","shell.execute_reply.started":"2024-08-18T12:03:06.548091Z","shell.execute_reply":"2024-08-18T12:03:06.559706Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"### Creating a Training Image Data Generator","metadata":{}},{"cell_type":"code","source":"color_mode = 'grayscale' if nr_of_channels == 1 else 'rgb'\n\n# Create a data generator with augmentation\ntrain_data_generator = ImageDataGenerator(\n    rescale=1./255,  # Rescale the pixel values (normalization)\n    rotation_range=15,  # Random rotation in the range of 15 degrees\n    width_shift_range=0.15,  # Random horizontal shifts (15% of total width)\n    height_shift_range=0.15,  # Random vertical shifts (15% of total height)\n    shear_range=0.15,  # Random shearing transformations\n    zoom_range=0.15,  # Random zoom range\n    horizontal_flip=True,  # Randomly flip inputs horizontally\n)\n\n# Load images from the directory and apply the defined transformations\nfer_training_data = train_data_generator.flow_from_directory(\n    os.path.join(dataset_path,'/train'),  # Path to the training data\n    target_size=dataset_size,\n    batch_size=batch_size,  # Number of images to yield per batch\n    color_mode=color_mode,\n    shuffle=True,\n    class_mode='categorical'  # Labels will be returned in categorical format\n)\n\nfer_training_data","metadata":{"execution":{"iopub.status.busy":"2024-08-18T12:03:06.562226Z","iopub.execute_input":"2024-08-18T12:03:06.563042Z","iopub.status.idle":"2024-08-18T12:03:07.813148Z","shell.execute_reply.started":"2024-08-18T12:03:06.563000Z","shell.execute_reply":"2024-08-18T12:03:07.810846Z"},"trusted":true},"execution_count":5,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[5], line 15\u001b[0m\n\u001b[1;32m      4\u001b[0m train_data_generator \u001b[38;5;241m=\u001b[39m ImageDataGenerator(\n\u001b[1;32m      5\u001b[0m     rescale\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.\u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m255\u001b[39m,  \u001b[38;5;66;03m# Rescale the pixel values (normalization)\u001b[39;00m\n\u001b[1;32m      6\u001b[0m     rotation_range\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m15\u001b[39m,  \u001b[38;5;66;03m# Random rotation in the range of 15 degrees\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     11\u001b[0m     horizontal_flip\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,  \u001b[38;5;66;03m# Randomly flip inputs horizontally\u001b[39;00m\n\u001b[1;32m     12\u001b[0m )\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# Load images from the directory and apply the defined transformations\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m fer_training_data \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_data_generator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflow_from_directory\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset_path\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/train\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Path to the training data\u001b[39;49;00m\n\u001b[1;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtarget_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdataset_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Number of images to yield per batch\u001b[39;49;00m\n\u001b[1;32m     19\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcolor_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolor_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m    \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclass_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcategorical\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Labels will be returned in categorical format\u001b[39;49;00m\n\u001b[1;32m     22\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m fer_training_data\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/keras/src/preprocessing/image.py:1648\u001b[0m, in \u001b[0;36mImageDataGenerator.flow_from_directory\u001b[0;34m(self, directory, target_size, color_mode, classes, class_mode, batch_size, shuffle, seed, save_to_dir, save_prefix, save_format, follow_links, subset, interpolation, keep_aspect_ratio)\u001b[0m\n\u001b[1;32m   1562\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mflow_from_directory\u001b[39m(\n\u001b[1;32m   1563\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1564\u001b[0m     directory,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1578\u001b[0m     keep_aspect_ratio\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m   1579\u001b[0m ):\n\u001b[1;32m   1580\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Takes the path to a directory & generates batches of augmented data.\u001b[39;00m\n\u001b[1;32m   1581\u001b[0m \n\u001b[1;32m   1582\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1646\u001b[0m \u001b[38;5;124;03m            and `y` is a numpy array of corresponding labels.\u001b[39;00m\n\u001b[1;32m   1647\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1648\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mDirectoryIterator\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1649\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdirectory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1650\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1651\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtarget_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtarget_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1652\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcolor_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolor_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1653\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkeep_aspect_ratio\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeep_aspect_ratio\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1654\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclasses\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclasses\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1655\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclass_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclass_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1656\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata_format\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1657\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1658\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshuffle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1659\u001b[0m \u001b[43m        \u001b[49m\u001b[43mseed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1660\u001b[0m \u001b[43m        \u001b[49m\u001b[43msave_to_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msave_to_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1661\u001b[0m \u001b[43m        \u001b[49m\u001b[43msave_prefix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msave_prefix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1662\u001b[0m \u001b[43m        \u001b[49m\u001b[43msave_format\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msave_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1663\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfollow_links\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfollow_links\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1664\u001b[0m \u001b[43m        \u001b[49m\u001b[43msubset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msubset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1665\u001b[0m \u001b[43m        \u001b[49m\u001b[43minterpolation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minterpolation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1666\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1667\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/keras/src/preprocessing/image.py:563\u001b[0m, in \u001b[0;36mDirectoryIterator.__init__\u001b[0;34m(self, directory, image_data_generator, target_size, color_mode, classes, class_mode, batch_size, shuffle, seed, data_format, save_to_dir, save_prefix, save_format, follow_links, subset, interpolation, keep_aspect_ratio, dtype)\u001b[0m\n\u001b[1;32m    561\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m classes:\n\u001b[1;32m    562\u001b[0m     classes \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m--> 563\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m subdir \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28msorted\u001b[39m(\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlistdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdirectory\u001b[49m\u001b[43m)\u001b[49m):\n\u001b[1;32m    564\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misdir(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(directory, subdir)):\n\u001b[1;32m    565\u001b[0m             classes\u001b[38;5;241m.\u001b[39mappend(subdir)\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/train'"],"ename":"FileNotFoundError","evalue":"[Errno 2] No such file or directory: '/train'","output_type":"error"}]},{"cell_type":"markdown","source":"### Creating a Test Image Data Generator","metadata":{}},{"cell_type":"code","source":"# Initialize an ImageDataGenerator for test data with rescaling\n# Rescales images by dividing pixel values by 255 (normalization)\ntest_data_generator = ImageDataGenerator(rescale=1./255)\n\n# Creates a data generator for the test dataset\nfer_test_data = test_data_generator.flow_from_directory(\n    os.path.join(dataset_path,'/test'),  # Directory path for test images\n    target_size=dataset_size,\n    batch_size = batch_size,  # Number of images to yield per batch\n    color_mode=color_mode,\n    shuffle=True,\n    class_mode = 'categorical'  # Images are classified categorically\n)\n\n# fer_test_data is now a generator that yields batches of test images and their labels\nfer_test_data","metadata":{"execution":{"iopub.status.busy":"2024-08-18T12:03:07.814003Z","iopub.status.idle":"2024-08-18T12:03:07.814434Z","shell.execute_reply.started":"2024-08-18T12:03:07.814231Z","shell.execute_reply":"2024-08-18T12:03:07.814252Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Importing the optimizers module from TensorFlow's Keras library\nfrom tensorflow.keras import optimizers\n\n# Initializing a list of optimizers with specific configurations\noptims = [\n    optimizers.Nadam(learning_rate=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-07, name='Nadam'),\n    optimizers.Adam(0.001),\n]","metadata":{"execution":{"iopub.status.busy":"2024-08-18T12:03:07.815979Z","iopub.status.idle":"2024-08-18T12:03:07.816367Z","shell.execute_reply.started":"2024-08-18T12:03:07.816183Z","shell.execute_reply":"2024-08-18T12:03:07.816201Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Defining the Model","metadata":{}},{"cell_type":"markdown","source":"### AlexNet model","metadata":{}},{"cell_type":"code","source":"from keras.models import Sequential\nfrom keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n\nstrategy = tf.distribute.MirroredStrategy()\nprint('Number of devices: {}'.format(strategy.num_replicas_in_sync))\n\nmerged_size = dataset_size + (nr_of_channels,)\n\nwith strategy.scope():\n\n    model = Sequential()\n\n    # Convolutional layers\n    model.add(Conv2D(96, kernel_size=(11,11), strides=(4,4), activation='relu', input_shape=merged_size, padding='same', kernel_initializer='he_normal', name='conv2d_1'))\n    model.add(MaxPooling2D(pool_size=(3,3), strides=(2,2), padding='same', name='maxpool2d_1'))\n    model.add(Conv2D(256, kernel_size=(5,5), activation='relu', padding='same', kernel_initializer='he_normal', name='conv2d_2'))\n    model.add(MaxPooling2D(pool_size=(3,3), strides=(2,2), padding='same', name='maxpool2d_2'))\n    model.add(Conv2D(384, kernel_size=(3,3), activation='relu', padding='same', kernel_initializer='he_normal', name='conv2d_3'))\n    model.add(Conv2D(384, kernel_size=(3,3), activation='relu', padding='same', kernel_initializer='he_normal', name='conv2d_4'))\n    model.add(Conv2D(256, kernel_size=(3,3), activation='relu', padding='same', kernel_initializer='he_normal', name='conv2d_5'))\n    model.add(MaxPooling2D(pool_size=(3,3), strides=(2,2), padding='same', name='maxpool2d_3'))\n\n    # Fully connected layers\n    model.add(Flatten(name='flatten'))\n    model.add(Dense(4096, activation='relu', kernel_initializer='he_normal', name='dense_1'))\n    model.add(Dropout(0.5, name='dropout_1'))\n    model.add(Dense(4096, activation='relu', kernel_initializer='he_normal', name='dense_2'))\n    model.add(Dropout(0.5, name='dropout_2'))\n    model.add(Dense(nr_classes, activation='softmax', name='out_layer'))\n\n    model.compile(loss='categorical_crossentropy', optimizer=optimizers.SGD(learning_rate=0.01, momentum=0.9), metrics=['accuracy'])\n\n    model.summary()\n","metadata":{"execution":{"iopub.status.busy":"2024-08-18T12:03:07.817644Z","iopub.status.idle":"2024-08-18T12:03:07.818045Z","shell.execute_reply.started":"2024-08-18T12:03:07.817835Z","shell.execute_reply":"2024-08-18T12:03:07.817853Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### CNN2D model","metadata":{}},{"cell_type":"code","source":"# from keras.models import Sequential\n# from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n\n# model = Sequential()\n\n# model.add(\n#         Conv2D(\n#             filters=512,\n#             kernel_size=(5,5),\n#             input_shape=merged_size,\n#             activation='elu',\n#             padding='same',\n#             kernel_initializer='he_normal',\n#             name='conv2d_1'\n#         )\n#     )\n# model.add(BatchNormalization(name='batchnorm_1'))\n# model.add(\n#         Conv2D(\n#             filters=256,\n#             kernel_size=(5,5),\n#             activation='elu',\n#             padding='same',\n#             kernel_initializer='he_normal',\n#             name='conv2d_2'\n#         )\n#     )\n# model.add(BatchNormalization(name='batchnorm_2'))\n    \n# model.add(MaxPooling2D(pool_size=(2,2), name='maxpool2d_1'))\n# model.add(Dropout(0.25, name='dropout_1'))\n\n# model.add(\n#         Conv2D(\n#             filters=128,\n#             kernel_size=(3,3),\n#             activation='elu',\n#             padding='same',\n#             kernel_initializer='he_normal',\n#             name='conv2d_3'\n#         )\n#     )\n# model.add(BatchNormalization(name='batchnorm_3'))\n# model.add(\n#         Conv2D(\n#             filters=128,\n#             kernel_size=(3,3),\n#             activation='elu',\n#             padding='same',\n#             kernel_initializer='he_normal',\n#             name='conv2d_4'\n#         )\n#     )\n# model.add(BatchNormalization(name='batchnorm_4'))\n    \n# model.add(MaxPooling2D(pool_size=(2,2), name='maxpool2d_2'))\n# model.add(Dropout(0.25, name='dropout_2'))\n\n# model.add(\n#         Conv2D(\n#             filters=256,\n#             kernel_size=(3,3),\n#             activation='elu',\n#             padding='same',\n#             kernel_initializer='he_normal',\n#             name='conv2d_5'\n#         )\n#     )\n# model.add(BatchNormalization(name='batchnorm_5'))\n# model.add(\n#         Conv2D(\n#             filters=512,\n#             kernel_size=(3,3),\n#             activation='elu',\n#             padding='same',\n#             kernel_initializer='he_normal',\n#             name='conv2d_6'\n#         )\n#     )\n# model.add(BatchNormalization(name='batchnorm_6'))\n    \n# model.add(MaxPooling2D(pool_size=(2,2), name='maxpool2d_3'))\n# model.add(Dropout(0.25, name='dropout_3'))\n\n# model.add(Flatten(name='flatten'))\n        \n# model.add(\n#         Dense(\n#             256,\n#             activation='elu',\n#             kernel_initializer='he_normal',\n#             name='dense_1'\n#         )\n#     )\n# model.add(BatchNormalization(name='batchnorm_7'))\n    \n# model.add(Dropout(0.25, name='dropout_4'))\n    \n# model.add(\n#         Dense(\n#             nr_classes,\n#             activation='softmax',\n#             name='out_layer'\n#         )\n#     )\n    \n# model.compile(\n#         loss='categorical_crossentropy',\n#         optimizer='adam',\n#         metrics=['accuracy']\n#     )\n    \n# model.summary()","metadata":{"execution":{"iopub.status.busy":"2024-08-18T12:03:07.819006Z","iopub.status.idle":"2024-08-18T12:03:07.819366Z","shell.execute_reply.started":"2024-08-18T12:03:07.819189Z","shell.execute_reply":"2024-08-18T12:03:07.819206Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Callbacks","metadata":{}},{"cell_type":"code","source":"# Learning Rate Scheduler\n# Reduce learning rate when a metric has stopped improving.\nlr_scheduler = ReduceLROnPlateau(\n    monitor='val_accuracy', # Quantity to be monitored.\n    factor=0.5, # Factor by which the learning rate will be reduced.\n    patience=7, # Number of epochs with no improvement after which learning rate will be reduced.\n    min_lr=1e-6, #  Lower bound on the learning rate.\n    verbose=1,  # Enables progress messages\n)\n\n# Model Checkpoint Callback\n# Saves the model after epoch if the achieved val_accuracy was the best of all epochs\ncheckpoint_period = ModelCheckpoint('./weights_ep{epoch:03d}.h5', # Path template to save the model weights\n                                    monitor='val_accuracy', # Monitors the validation accuracy\n                                    save_weights_only=True, # Only saves the model's weights\n                                    save_best_only=True, # Only saves the best model\n                                    save_freq='epoch') # Saves the model at the end of every epoch\n\n# List of callbacks to be passed to the model during training\ncallbacks = [\n    lr_scheduler,\n    checkpoint_period\n]","metadata":{"execution":{"iopub.status.busy":"2024-08-18T12:03:07.820811Z","iopub.status.idle":"2024-08-18T12:03:07.821198Z","shell.execute_reply.started":"2024-08-18T12:03:07.821025Z","shell.execute_reply":"2024-08-18T12:03:07.821042Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Training the Model","metadata":{}},{"cell_type":"code","source":"# Train the model with the specified parameters and data\nhistory = model.fit(\n    fer_training_data, # Training data generator\n    epochs=nr_epochs, # Number of epochs\n    validation_data=fer_test_data, # Validation data generator\n    batch_size = batch_size,\n    callbacks=callbacks,\n)","metadata":{"execution":{"iopub.status.busy":"2024-08-18T12:03:07.823172Z","iopub.status.idle":"2024-08-18T12:03:07.823716Z","shell.execute_reply.started":"2024-08-18T12:03:07.823439Z","shell.execute_reply":"2024-08-18T12:03:07.823465Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Visualizing Loss and Accuracy","metadata":{}},{"cell_type":"code","source":"import plotly.graph_objects as go\n\n# Create traces\nfig = go.Figure()\n\n# Training vs. Validation Accuracy\nfig.add_trace(go.Scatter(x=list(range(1, len(history.history['accuracy']) + 1)), y=history.history['accuracy'], mode='lines+markers', name='Training Accuracy'))\nfig.add_trace(go.Scatter(x=list(range(1, len(history.history['val_accuracy']) + 1)), y=history.history['val_accuracy'], mode='lines+markers', name='Validation Accuracy'))\n\n# Layout for Accuracy\nfig.update_layout(title='Training vs. Validation Accuracy', xaxis_title='Epoch', yaxis_title='Accuracy', template=\"plotly_white\")\n\n# Show the plot\nfig.show()\n\n# New figure for loss\nfig = go.Figure()\n\n# Training vs. Validation Loss\nfig.add_trace(go.Scatter(x=list(range(1, len(history.history['loss']) + 1)), y=history.history['loss'], mode='lines+markers', name='Training Loss'))\nfig.add_trace(go.Scatter(x=list(range(1, len(history.history['val_loss']) + 1)), y=history.history['val_loss'], mode='lines+markers', name='Validation Loss'))\n\n# Layout for Loss\nfig.update_layout(title='Training vs. Validation Loss', xaxis_title='Epoch', yaxis_title='Loss', template=\"plotly_white\")\n\n# Show the plot\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2024-08-18T12:03:07.824750Z","iopub.status.idle":"2024-08-18T12:03:07.825241Z","shell.execute_reply.started":"2024-08-18T12:03:07.824992Z","shell.execute_reply":"2024-08-18T12:03:07.825014Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Restoores the weights to the checkpoint with best accuracy","metadata":{}},{"cell_type":"code","source":"# Directory where the checkpoint files are located\ncheckpoint_dir = \"/kaggle/working\"\n\n# Define a function to extract the epoch number from a file name\ndef extract_epoch(file_name):\n    return int(file_name.split(\"_\")[1][2:-3])  # Assuming the file format is \"weights_epXXX.h5\"\n\n# Get a list of all files in the directory\nfiles = os.listdir(checkpoint_dir)\n\n# Filter out only the checkpoint files\ncheckpoint_files = [file for file in files if file.startswith(\"weights_ep\") and file.endswith(\".h5\")]\n\nif not checkpoint_files:\n    print(\"No checkpoint files found.\")\nelse:\n    # Find the file with the highest epoch number\n    highest_epoch_file = max(checkpoint_files, key=extract_epoch)\n    print(highest_epoch_file)\n    model.load_weights(os.path.join(checkpoint_dir, highest_epoch_file))\n","metadata":{"execution":{"iopub.status.busy":"2024-08-18T12:03:07.827121Z","iopub.status.idle":"2024-08-18T12:03:07.827484Z","shell.execute_reply.started":"2024-08-18T12:03:07.827304Z","shell.execute_reply":"2024-08-18T12:03:07.827333Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Print some example recognitions","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport numpy as np\n\n\n# Needs to be modified depending of used dataset\nif nr_classes == 7:\n    label_map = {\n        0: \"calm\",\n        1: \"happy\",\n        2: \"sad\",\n        3: \"angry\",\n        4: \"fearful\",\n        5: \"disgust\",\n        6: \"surprised\"\n    }\nelif nr_classes == 8:\n    label_map = {\n        0: \"neutral\",\n        1: \"calm\",\n        2: \"happy\",\n        3: \"sad\",\n        4: \"angry\",\n        5: \"fearful\",\n        6: \"disgust\",\n        7: \"surprised\"\n    }    \nelif nr_classes == 6:\n    label_map = {\n        1: \"calm\",\n        2: \"happy\",\n        3: \"sad\",\n        4: \"angry\",\n        5: \"fearful\",\n        6: \"surprised\"\n    }    \nelse:\n    label_map = {\n        0: \"confussion\",\n        1: \"happines\",\n        2: \"neutral\",\n        3: \"surprise\"\n    }\n\n# Get a batch of images and labels from the test dataset\nbatch_images, batch_labels = next(iter(fer_test_data))\n\n# Make predictions using the trained model\nbatch_predictions = model.predict(batch_images)\nbatch_predicted_labels = np.argmax(batch_predictions, axis=1)\n\n# Plot example images with true and predicted labels\nplt.figure(figsize=(15, 8))\nfor i in range(5):  # Plot 5 example images\n    plt.subplot(1, 5, i + 1)\n    plt.imshow(batch_images[i])\n    true_label =  label_map[np.argmax(batch_labels[i])]\n    predicted_label = label_map[batch_predicted_labels[i]]\n    plt.title(f'True: {true_label}\\nPredicted: {predicted_label}')\n    plt.axis('off')\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2024-08-18T12:03:07.828766Z","iopub.status.idle":"2024-08-18T12:03:07.829566Z","shell.execute_reply.started":"2024-08-18T12:03:07.829365Z","shell.execute_reply":"2024-08-18T12:03:07.829385Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Evaluate the model on the test dataset\nloss, accuracy = model.evaluate(fer_test_data, verbose=0)\nprint(f'Test accuracy: {accuracy}')","metadata":{"execution":{"iopub.status.busy":"2024-08-18T12:03:07.830591Z","iopub.status.idle":"2024-08-18T12:03:07.830966Z","shell.execute_reply.started":"2024-08-18T12:03:07.830764Z","shell.execute_reply":"2024-08-18T12:03:07.830781Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Print confussion matrix","metadata":{}},{"cell_type":"code","source":"import numpy as np\nfrom sklearn.metrics import confusion_matrix\nimport seaborn as sns\n\n# Initialize empty lists to store all predictions and true labels\nall_predictions = []\nall_true_labels = []\n\n# Calculate the total number of batches\ntotal_batches = len(fer_test_data)\n\n# Iterate over all batches in the test dataset\nfor batch_index in range(total_batches):\n    batch_images, batch_labels = fer_test_data[batch_index]\n    # Make predictions using the trained model for the current batch\n    batch_predictions = model.predict(batch_images)\n    # Get predicted labels for the batch\n    batch_predicted_labels = np.argmax(batch_predictions, axis=1)\n    # Convert batch_labels to numpy array and get the true labels\n    batch_true_labels = np.argmax(np.array(batch_labels), axis=1)\n    # Append the batch predictions and true labels to the respective lists\n    all_predictions.extend(batch_predicted_labels)\n    all_true_labels.extend(batch_true_labels)\n\n# Convert lists to numpy arrays\nall_predictions = np.array(all_predictions)\nall_true_labels = np.array(all_true_labels)\n\n# Calculate confusion matrix\nconf_matrix = confusion_matrix(all_true_labels, all_predictions)\n\n# Plot confusion matrix\nplt.figure(figsize=(10, 8))\nsns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=label_map.values(), yticklabels=label_map.values())\nplt.xlabel('Predicted labels')\nplt.ylabel('True labels')\nplt.title('Confusion Matrix')\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2024-08-18T12:03:07.832560Z","iopub.status.idle":"2024-08-18T12:03:07.832941Z","shell.execute_reply.started":"2024-08-18T12:03:07.832733Z","shell.execute_reply":"2024-08-18T12:03:07.832749Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Save model to h5 file and crate zip link to download","metadata":{}},{"cell_type":"code","source":"model.save('/kaggle/working/model.h5')","metadata":{"execution":{"iopub.status.busy":"2024-08-18T12:03:07.834691Z","iopub.status.idle":"2024-08-18T12:03:07.835056Z","shell.execute_reply.started":"2024-08-18T12:03:07.834854Z","shell.execute_reply":"2024-08-18T12:03:07.834889Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nfrom zipfile import ZipFile\nfrom IPython.display import FileLink\n\n# Directory containing the file to be downloaded\ndirectory_to_compress = '/kaggle/working'\n\n# Name of the file to be downloaded\nfile_to_download = 'model.h5'  \n\n# Temporary zip file path\nzip_file_path = 'model.zip'\n\n# Compress the specific file into a zip file\nwith ZipFile(zip_file_path, 'w') as zipf:\n    zipf.write(os.path.join(directory_to_compress, file_to_download), file_to_download)\n\n# Display download link for the zip file containing the specific file\nFileLink(zip_file_path)","metadata":{"execution":{"iopub.status.busy":"2024-08-18T12:03:07.836532Z","iopub.status.idle":"2024-08-18T12:03:07.836931Z","shell.execute_reply.started":"2024-08-18T12:03:07.836719Z","shell.execute_reply":"2024-08-18T12:03:07.836737Z"},"trusted":true},"execution_count":null,"outputs":[]}]}